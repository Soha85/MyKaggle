{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f69f55",
   "metadata": {
    "papermill": {
     "duration": 0.008649,
     "end_time": "2024-02-10T17:21:51.842440",
     "exception": false,
     "start_time": "2024-02-10T17:21:51.833791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05d9877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:21:51.860216Z",
     "iopub.status.busy": "2024-02-10T17:21:51.859574Z",
     "iopub.status.idle": "2024-02-10T17:22:07.199040Z",
     "shell.execute_reply": "2024-02-10T17:22:07.197982Z"
    },
    "papermill": {
     "duration": 15.351078,
     "end_time": "2024-02-10T17:22:07.201439",
     "exception": false,
     "start_time": "2024-02-10T17:21:51.850361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist, MLEProbDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc0e8f",
   "metadata": {
    "papermill": {
     "duration": 0.007496,
     "end_time": "2024-02-10T17:22:07.216973",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.209477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main breakthroughs in word embeddings, from word counts to LLMs:\n",
    "\n",
    "**Low-tech beginnings:**\n",
    "\n",
    "* **Word Counts:** Basic frequency of words in a corpus served as initial representations. Useful for simple tasks but ignored semantic relationships.\n",
    "* **Term Frequency (TF):** As mentioned, simply counting the occurrences of words within a corpus was one of the earliest methods for representing words. This basic approach laid the foundation for more sophisticated techniques.\n",
    "* **N-grams:** These are sequences of n consecutive words (e.g., bigrams for pairs, trigrams for triplets). By analyzing n-gram frequencies, we can capture some local context and word relationships, going beyond individual word counts.\n",
    "\n",
    "\n",
    "**Distributional Semantics:**\n",
    "\n",
    "* **Word2Vec (2013):** First major breakthrough. Learned word embeddings by predicting surrounding words, capturing semantic similarities.\n",
    "* **GloVe (2014):** Leverages co-occurrence statistics for better context sensitivity.\n",
    "* **FastText (2016):** Incorporates subword information, handling rare words and morphological variants.\n",
    "\n",
    "**Contextual Embeddings:**\n",
    "\n",
    "* **ELMo (2018):** Uses bi-directional LSTMs to capture word meaning based on surrounding context.\n",
    "* **BERT (2018):** Pre-trained transformer model on large unlabeled text, learning contextualized representations.\n",
    "* **XLNet (2019):** Builds upon BERT's masked language modeling with permutation language modeling for better understanding of word relationships.\n",
    "\n",
    "**Towards Understanding and Generation:**\n",
    "\n",
    "* **GPT-3 (2020):** Generative Pre-trained Transformer 3, a large language model (LLM) with impressive text generation capabilities.\n",
    "* **LaMDA (2021):** Language Model for Dialogue Applications, focuses on factual consistency and grounding in conversation.\n",
    "* **PaLM (2022):** Pathways Language Model, pushes the boundaries of LLM size and performance, demonstrating progress in reasoning and question answering.\n",
    "\n",
    "**Key breakpoints:**\n",
    "\n",
    "* **From word counts to context:** Moving beyond simple frequency to considering surrounding words for richer representations.\n",
    "* **Pre-training on large corpora:** Utilizing massive amounts of text data to learn general language understanding.\n",
    "* **Bi-directional and attention mechanisms:** Capturing complex relationships between words in a sentence.\n",
    "* **Transformers and self-attention:** Enabling efficient learning of long-range dependencies.\n",
    "* **LLMs reaching human-level performance in certain tasks:** Highlighting the potential of language models for natural communication and problem-solving.\n",
    "\n",
    "**Future directions:**\n",
    "\n",
    "* **Explainability and interpretability:** Understanding how LLMs work and make decisions.\n",
    "* **Addressing biases and fairness:** Ensuring models are inclusive and represent diverse perspectives.\n",
    "* **Combining symbolic and neural approaches:** Integrating logic and reasoning with language understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf50c35",
   "metadata": {
    "papermill": {
     "duration": 0.007401,
     "end_time": "2024-02-10T17:22:07.232187",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.224786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **Count of Words (Bag-of-Words Model):**  Early NLP models represented documents using a Bag-of-Words (BoW) model, which counts the occurrence of words in a document without considering their order. This approach provides a basic representation of documents but lacks capturing semantic relationships.\n",
    "* **Term Frequency-Inverse Document Frequency (TF-IDF):** introduced a weighting scheme that considers not only the count of words in a document but also their importance in the entire corpus. It helps identify words that are significant to a particular document but not frequent across all documents.\n",
    "* **Latent Semantic Analysis (LSA):** also known as Latent Semantic Indexing (LSI), applies singular value decomposition (SVD) to the term-document matrix. It reduces the dimensionality of the space and captures latent semantic relationships between words and documents, improving representation.\n",
    "* **Skip-gram and Continuous Bag of Words (CBOW):** Word2Vec, introduced by Mikolov et al., includes two models: Skip-gram and Continuous Bag of Words (CBOW). These models use shallow neural networks to learn word embeddings. Skip-gram predicts context words given a target word, while CBOW predicts a target word given its context. Word2Vec significantly improves word embeddings' quality and captures semantic relationships.\n",
    "* **Global Vectors for Word Representation (GloVe):** introduces a global approach by training on aggregated global word co-occurrence statistics. It leverages a matrix factorization technique to capture the relationships between words in a more efficient manner, producing high-quality word embeddings.\n",
    "* **FastText and Subword Embeddings:** also by Mikolov et al., extends word embeddings to subword level. It represents words as bags of character n-grams, enabling the generation of embeddings for out-of-vocabulary words and capturing morphological information.\n",
    "* **Transformer Architecture:** introduced by Vaswani et al., revolutionizes NLP by employing self-attention mechanisms. It enables models like BERT (Bidirectional Encoder Representations from Transformers) to learn contextualized word embeddings, considering the entire input sequence bidirectionally.\n",
    "* **Contextualized Word Embeddings (BERT, GPT, ELMo):** BERT, GPT (Generative Pre-trained Transformer), and ELMo (Embeddings from Language Models) introduce contextualized embeddings by considering surrounding words and contexts. These models capture rich contextual information, leading to state-of-the-art performance in various NLP tasks.\n",
    "* **Transfer Learning and Fine-Tuning:** Pre-trained language models, such as BERT and GPT, can be fine-tuned on specific downstream tasks. This transfer learning approach significantly reduces the need for large labeled datasets and improves performance on specific tasks.\n",
    "* **Multimodal Embeddings:** are extended beyond text to include multimodal information, combining textual and visual features. Models like CLIP (Contrastive Language-Image Pre-training) learn joint representations of text and images, enabling cross-modal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f81a85",
   "metadata": {
    "papermill": {
     "duration": 0.007368,
     "end_time": "2024-02-10T17:22:07.247225",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.239857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Documents list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca8abac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.264479Z",
     "iopub.status.busy": "2024-02-10T17:22:07.263595Z",
     "iopub.status.idle": "2024-02-10T17:22:07.268671Z",
     "shell.execute_reply": "2024-02-10T17:22:07.267646Z"
    },
    "papermill": {
     "duration": 0.015791,
     "end_time": "2024-02-10T17:22:07.270671",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.254880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Doc_1= \"The cat in the hat\"\n",
    "Doc_2= \"The quick brown fox\"\n",
    "Doc_3= \"The hat is blue\"\n",
    "\n",
    "Docs =[Doc_1,Doc_2,Doc_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af19cab",
   "metadata": {
    "papermill": {
     "duration": 0.007406,
     "end_time": "2024-02-10T17:22:07.285799",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.278393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Term Frequency (TF)**\n",
    "* **TF(t,d) is the term frequency of term t in document d (how often the term appears in the document).*** **TF(t,d) is the term frequency of term t in document d (how often the term appears in the document).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb503d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.302776Z",
     "iopub.status.busy": "2024-02-10T17:22:07.302400Z",
     "iopub.status.idle": "2024-02-10T17:22:07.310522Z",
     "shell.execute_reply": "2024-02-10T17:22:07.309355Z"
    },
    "papermill": {
     "duration": 0.019257,
     "end_time": "2024-02-10T17:22:07.312683",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.293426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blue', 'brown', 'cat', 'fox', 'hat', 'in', 'is', 'quick', 'the'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get distinct words for each document\n",
    "lst = []\n",
    "for d in Docs:\n",
    "    lst.extend(d.lower().split(' '))\n",
    "wrds = set(lst) # remove duplicate words\n",
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9377d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.330358Z",
     "iopub.status.busy": "2024-02-10T17:22:07.329997Z",
     "iopub.status.idle": "2024-02-10T17:22:07.367010Z",
     "shell.execute_reply": "2024-02-10T17:22:07.366289Z"
    },
    "papermill": {
     "duration": 0.048308,
     "end_time": "2024-02-10T17:22:07.369032",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.320724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hat   the  brown  cat   in  quick  blue    is   fox\n",
       "0  0.20  0.40   0.00  0.2  0.2   0.00  0.00  0.00  0.00\n",
       "1  0.00  0.25   0.25  0.0  0.0   0.25  0.00  0.00  0.25\n",
       "2  0.25  0.25   0.00  0.0  0.0   0.00  0.25  0.25  0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#form a dataframe to represent TF for each word in each Document where columns are words and rows are documents\n",
    "\n",
    "def count_wrd_Doc(wrd,doc):\n",
    "    i=0\n",
    "    for w in doc.lower().split(' '):\n",
    "        if wrd == w:\n",
    "            i = i+1\n",
    "    return i/len(doc.lower().split(' '))\n",
    "    \n",
    "tf_df = pd.DataFrame(columns=list(wrds)) #empty dataframe initialized with words column headers\n",
    "freq_lst=[] #empty list for each column to save word frequencies in each document\n",
    "for c in tf_df.columns:\n",
    "    freq_lst=[]#empty the list\n",
    "    for d in Docs:\n",
    "        freq_lst.append(count_wrd_Doc(c,d))#append the frequency of word in document d\n",
    "    tf_df[c]=freq_lst #assign values to column\n",
    "tf_df #display the dataframe of TF for each word in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97067b",
   "metadata": {
    "papermill": {
     "duration": 0.008325,
     "end_time": "2024-02-10T17:22:07.385425",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.377100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document Frequency (DF)\n",
    "* **Calculate Document Frequency (DF): the word appears in how many documents*** **Calculate Document Frequency (DF): the word appears in how many documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad34c166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.403450Z",
     "iopub.status.busy": "2024-02-10T17:22:07.403027Z",
     "iopub.status.idle": "2024-02-10T17:22:07.419555Z",
     "shell.execute_reply": "2024-02-10T17:22:07.418698Z"
    },
    "papermill": {
     "duration": 0.028099,
     "end_time": "2024-02-10T17:22:07.421667",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.393568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hat  the  brown  cat  in  quick  blue  is  fox\n",
       "0    2    3      1    1   1      1     1   1    1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_df = pd.DataFrame(columns=list(wrds)) #empty dataframe initialized with words column headers\n",
    "for c in df_df.columns:\n",
    "    df_df[c] = [sum(1 for doc in Docs if c in doc.lower().split(' '))]\n",
    "df_df #display the dataframe of DF for each word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47b3fb",
   "metadata": {
    "papermill": {
     "duration": 0.008293,
     "end_time": "2024-02-10T17:22:07.438675",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.430382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inverse Document Frequency (IDF)\n",
    "* **IDF(t,D) is the inverse document frequency of term t in the entire document set D (logarithmically scaled inverse fraction of the documents that contain the term).*** **IDF(t,D) is the inverse document frequency of term t in the entire document set D (logarithmically scaled inverse fraction of the documents that contain the term).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f7acad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.457539Z",
     "iopub.status.busy": "2024-02-10T17:22:07.456833Z",
     "iopub.status.idle": "2024-02-10T17:22:07.474905Z",
     "shell.execute_reply": "2024-02-10T17:22:07.473823Z"
    },
    "papermill": {
     "duration": 0.029823,
     "end_time": "2024-02-10T17:22:07.477108",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.447285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hat  the     brown       cat        in     quick      blue        is  \\\n",
       "0  1.287682  1.0  1.693147  1.693147  1.693147  1.693147  1.693147  1.693147   \n",
       "\n",
       "        fox  \n",
       "0  1.693147  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_df = pd.DataFrame(columns=list(wrds)) #empty dataframe initialized with words column headers\n",
    "for c in idf_df.columns:\n",
    "    N = 3 #No of documents\n",
    "    df = df_df[c].iloc[0] # DF of word\n",
    "    idf_df[c] = [math.log((N+1) / (df+1))+1]#IDF = log (no. of documents/DF(word)) \n",
    "idf_df #display the dataframe of idf for each word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89603a73",
   "metadata": {
    "papermill": {
     "duration": 0.007982,
     "end_time": "2024-02-10T17:22:07.493424",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.485442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "**TF-IDF = TF * IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76df723d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.511661Z",
     "iopub.status.busy": "2024-02-10T17:22:07.511261Z",
     "iopub.status.idle": "2024-02-10T17:22:07.532693Z",
     "shell.execute_reply": "2024-02-10T17:22:07.531534Z"
    },
    "papermill": {
     "duration": 0.033216,
     "end_time": "2024-02-10T17:22:07.534816",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.501600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257536</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338629</td>\n",
       "      <td>0.338629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.423287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321921</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423287</td>\n",
       "      <td>0.423287</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hat   the     brown       cat        in     quick      blue        is  \\\n",
       "0  0.257536  0.40  0.000000  0.338629  0.338629  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.25  0.423287  0.000000  0.000000  0.423287  0.000000  0.000000   \n",
       "2  0.321921  0.25  0.000000  0.000000  0.000000  0.000000  0.423287  0.423287   \n",
       "\n",
       "        fox  \n",
       "0  0.000000  \n",
       "1  0.423287  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(columns=list(wrds)) #empty dataframe initialized with words column headers\n",
    "tfidf_lst=[]  #empty list for each column \n",
    "for c in tfidf_df.columns:\n",
    "    tfidf_lst=[] #empty list for each column\n",
    "    for i in range(0,len(Docs)):\n",
    "        tf_idf_d1 = tf_df[c].iloc[i]*idf_df[c].iloc[0] #append tf of word in i th document to idf of word\n",
    "        tfidf_lst.append(tf_idf_d1)\n",
    "    tfidf_df[c]=tfidf_lst#assign tfidf values for each word\n",
    "tfidf_df #display the dataframe of tf-idf for all words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edc6ff",
   "metadata": {
    "papermill": {
     "duration": 0.008296,
     "end_time": "2024-02-10T17:22:07.551613",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.543317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**L2 Normalization**\n",
    "* **L2 normalization, also known as Euclidean normalization or L2 norm normalization, is a technique used to scale vectors (or arrays) in such a way that their Euclidean norm becomes equal to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8491c484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.570692Z",
     "iopub.status.busy": "2024-02-10T17:22:07.569548Z",
     "iopub.status.idle": "2024-02-10T17:22:07.596019Z",
     "shell.execute_reply": "2024-02-10T17:22:07.595000Z"
    },
    "papermill": {
     "duration": 0.038429,
     "end_time": "2024-02-10T17:22:07.598348",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.559919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.592567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322745</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444514</td>\n",
       "      <td>0.345205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hat       the     brown       cat        in     quick      blue  \\\n",
       "0  0.381519  0.592567  0.000000  0.501651  0.501651  0.000000  0.000000   \n",
       "1  0.000000  0.322745  0.546454  0.000000  0.000000  0.546454  0.000000   \n",
       "2  0.444514  0.345205  0.000000  0.000000  0.000000  0.000000  0.584483   \n",
       "\n",
       "         is       fox  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.546454  \n",
       "2  0.584483  0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df = pd.DataFrame(columns=tfidf_df.columns)\n",
    "\n",
    "# Apply L2 normalization to each document's TF-IDF values\n",
    "for i,row in enumerate(tfidf_df.iterrows()):\n",
    "    # Extract TF-IDF values    \n",
    "    tfidf_values_list = list(tfidf_df.iloc[i].values)\n",
    "    # Calculate L2 norm\n",
    "    l2_norm = math.sqrt(sum(val**2 for val in tfidf_values_list))\n",
    "    # Normalize TF-IDF values using L2 norm\n",
    "    normalized_tfidf = [val / l2_norm for val in list(tfidf_df.iloc[i].values)]\n",
    "    new_row = pd.Series(normalized_tfidf, index=tfidf_df.columns)\n",
    "    normalized_df.loc[len(normalized_df)] = new_row\n",
    "    \n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01ff16",
   "metadata": {
    "papermill": {
     "duration": 0.008643,
     "end_time": "2024-02-10T17:22:07.615838",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.607195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TfidfVectorizer Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4b7aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.635486Z",
     "iopub.status.busy": "2024-02-10T17:22:07.634840Z",
     "iopub.status.idle": "2024-02-10T17:22:07.661816Z",
     "shell.execute_reply": "2024-02-10T17:22:07.660815Z"
    },
    "papermill": {
     "duration": 0.039555,
     "end_time": "2024-02-10T17:22:07.664159",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.624604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>fox</th>\n",
       "      <th>hat</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>quick</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546454</td>\n",
       "      <td>0.322745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.584483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       blue     brown       cat       fox       hat        in        is  \\\n",
       "0  0.000000  0.000000  0.501651  0.000000  0.381519  0.501651  0.000000   \n",
       "1  0.000000  0.546454  0.000000  0.546454  0.000000  0.000000  0.000000   \n",
       "2  0.584483  0.000000  0.000000  0.000000  0.444514  0.000000  0.584483   \n",
       "\n",
       "      quick       the  \n",
       "0  0.000000  0.592567  \n",
       "1  0.546454  0.322745  \n",
       "2  0.000000  0.345205  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2',smooth_idf=True)\n",
    "\n",
    "# Fit the documents and transform them into a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(Docs)\n",
    "\n",
    "# Get the feature names (terms) from the vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "df_tfidf = pd.DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8e51e",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.008839,
     "end_time": "2024-02-10T17:22:07.682219",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.673380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **The TfidfVectorizer in scikit-learn, by default, adds a smoothing term to the denominator of the IDF calculation to avoid division by zero. This is done to handle the case where a term is present in all documents, ensuring that the IDF is not undefined.**\n",
    "* **L2 normalization, also known as Euclidean normalization or L2 norm normalization, is a technique used to scale vectors (or arrays) in such a way that their Euclidean norm becomes equal to 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ff722",
   "metadata": {
    "papermill": {
     "duration": 0.009041,
     "end_time": "2024-02-10T17:22:07.700178",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.691137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unigram\n",
    "* **A unigram, in the context of natural language processing (NLP) and linguistics, refers to a single unit or token of a word. It is the simplest form of linguistic analysis where text is broken down into individual words. In other words, a unigram is a term used to describe a single word in a sequence of words.**\n",
    "* **Unigrams are the building blocks for more complex linguistic analyses, such as bigrams (pairs of consecutive words), trigrams (triplets of consecutive words), and n-grams in general.*** **A unigram, in the context of natural language processing (NLP) and linguistics, refers to a single unit or token of a word. It is the simplest form of linguistic analysis where text is broken down into individual words. In other words, a unigram is a term used to describe a single word in a sequence of words.**\n",
    "* **Unigrams are the building blocks for more complex linguistic analyses, such as bigrams (pairs of consecutive words), trigrams (triplets of consecutive words), and n-grams in general.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6002e4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.720489Z",
     "iopub.status.busy": "2024-02-10T17:22:07.719356Z",
     "iopub.status.idle": "2024-02-10T17:22:07.736631Z",
     "shell.execute_reply": "2024-02-10T17:22:07.735539Z"
    },
    "papermill": {
     "duration": 0.029774,
     "end_time": "2024-02-10T17:22:07.738895",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.709121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hat  the  brown  cat  in  quick  blue  is  fox\n",
       "0    1    2      0    1   1      0     0   0    0\n",
       "1    0    1      1    0   0      1     0   0    1\n",
       "2    1    1      0    0   0      0     1   1    0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Probability of unigram P(w)=C(w)/m same idea of TF\n",
    "def count_wrd_Doc(wrd,doc):\n",
    "    i=0\n",
    "    for w in doc.lower().split(' '):\n",
    "        if wrd == w:\n",
    "            i = i+1\n",
    "    return i\n",
    "    \n",
    "unigram_df = pd.DataFrame(columns=list(wrds)) #empty dataframe initialized with words column headers\n",
    "freq_lst=[] #empty list for each column to save word frequencies in each document\n",
    "for c in tf_df.columns:\n",
    "    freq_lst=[]#empty the list\n",
    "    for d in Docs:\n",
    "        freq_lst.append(count_wrd_Doc(c,d))#append the frequency of word in document d\n",
    "    unigram_df[c]=freq_lst #assign values to column\n",
    "unigram_df #display the dataframe of TF for each word in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991cefab",
   "metadata": {
    "papermill": {
     "duration": 0.008964,
     "end_time": "2024-02-10T17:22:07.757027",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.748063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unigrams python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d993288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.836247Z",
     "iopub.status.busy": "2024-02-10T17:22:07.835316Z",
     "iopub.status.idle": "2024-02-10T17:22:07.854953Z",
     "shell.execute_reply": "2024-02-10T17:22:07.853665Z"
    },
    "papermill": {
     "duration": 0.089008,
     "end_time": "2024-02-10T17:22:07.857310",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.768302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the',): 2\n",
      "('cat',): 1\n",
      "('in',): 1\n",
      "('hat',): 1\n",
      "('the',): 1\n",
      "('quick',): 1\n",
      "('brown',): 1\n",
      "('fox',): 1\n",
      "('the',): 1\n",
      "('hat',): 1\n",
      "('is',): 1\n",
      "('blue',): 1\n"
     ]
    }
   ],
   "source": [
    "for d in Docs:\n",
    "    words = word_tokenize(d.lower())\n",
    "    result = list(ngrams(words, 1))\n",
    "    # Calculate frequency distribution of bigrams\n",
    "    ngram_freq = FreqDist(result)\n",
    "    for word, frequency in ngram_freq.items():\n",
    "        print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ee67e",
   "metadata": {
    "papermill": {
     "duration": 0.008945,
     "end_time": "2024-02-10T17:22:07.875763",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.866818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bigram\n",
    "* **A bigram, in the context of natural language processing (NLP) and linguistics, refers to an ordered pair of consecutive words within a text or sequence of words. It is a type of n-gram, where \"n\" represents the number of words in the sequence.*** **A bigram, in the context of natural language processing (NLP) and linguistics, refers to an ordered pair of consecutive words within a text or sequence of words. It is a type of n-gram, where \"n\" represents the number of words in the sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485f04d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.896000Z",
     "iopub.status.busy": "2024-02-10T17:22:07.895637Z",
     "iopub.status.idle": "2024-02-10T17:22:07.924801Z",
     "shell.execute_reply": "2024-02-10T17:22:07.923585Z"
    },
    "papermill": {
     "duration": 0.04187,
     "end_time": "2024-02-10T17:22:07.926868",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.884998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the cat</th>\n",
       "      <th>cat in</th>\n",
       "      <th>in the</th>\n",
       "      <th>the hat</th>\n",
       "      <th>the quick</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>the hat</th>\n",
       "      <th>hat is</th>\n",
       "      <th>is blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the cat  cat in  in the  the hat  the quick  quick brown  brown fox  \\\n",
       "0        1       1       1        1          0            0          0   \n",
       "1        0       0       0        0          1            1          1   \n",
       "2        0       0       0        1          0            0          0   \n",
       "\n",
       "   the hat  hat is  is blue  \n",
       "0        1       0        0  \n",
       "1        0       0        0  \n",
       "2        1       1        1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get bi-grams of input sentence\n",
    "def bi_lst(doc):\n",
    "    wrds = doc.lower().split(' ')\n",
    "    bi_lst = []\n",
    "    for j in range(0,len(wrds)-1):\n",
    "        bi_lst.append(wrds[j:j+2])\n",
    "    return bi_lst\n",
    "\n",
    "lst = []\n",
    "for d in Docs:\n",
    "    lst.extend(bi_lst(d))\n",
    "unique_list = []\n",
    "unique_list = [item for item in lst if item not in unique_list]\n",
    "\n",
    "def count_biwrd_Doc(st,doc):\n",
    "    i=0    \n",
    "    for s in bi_lst(doc):\n",
    "        if s == st.split(' '):\n",
    "            i = i+1\n",
    "    return i\n",
    "bigram_df = pd.DataFrame(columns=list((' '.join(x) for x in unique_list))) #empty dataframe initialized with words column headers\n",
    "freq_lst=[] #empty list for each column to save word frequencies in each document\n",
    "for c in bigram_df.columns:\n",
    "    freq_lst=[]#empty the list\n",
    "    for d in Docs:\n",
    "        freq_lst.append(count_biwrd_Doc(c,d))#append the frequency of word in document d\n",
    "    bigram_df[c]=freq_lst #assign values to column\n",
    "bigram_df #display the dataframe of TF for each word in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25157756",
   "metadata": {
    "papermill": {
     "duration": 0.00944,
     "end_time": "2024-02-10T17:22:07.945886",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.936446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bigrams python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4604eeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:07.966939Z",
     "iopub.status.busy": "2024-02-10T17:22:07.966299Z",
     "iopub.status.idle": "2024-02-10T17:22:07.973514Z",
     "shell.execute_reply": "2024-02-10T17:22:07.972060Z"
    },
    "papermill": {
     "duration": 0.020298,
     "end_time": "2024-02-10T17:22:07.975687",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.955389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'cat'): 1\n",
      "('cat', 'in'): 1\n",
      "('in', 'the'): 1\n",
      "('the', 'hat'): 1\n",
      "('the', 'quick'): 1\n",
      "('quick', 'brown'): 1\n",
      "('brown', 'fox'): 1\n",
      "('the', 'hat'): 1\n",
      "('hat', 'is'): 1\n",
      "('is', 'blue'): 1\n"
     ]
    }
   ],
   "source": [
    "for d in Docs:\n",
    "    words = word_tokenize(d.lower())\n",
    "    result = list(ngrams(words, 2))\n",
    "    # Calculate frequency distribution of bigrams\n",
    "    ngram_freq = FreqDist(result)\n",
    "    for word, frequency in ngram_freq.items():\n",
    "        print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30d761",
   "metadata": {
    "papermill": {
     "duration": 0.009539,
     "end_time": "2024-02-10T17:22:07.995020",
     "exception": false,
     "start_time": "2024-02-10T17:22:07.985481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Skip-gram & Continuous Bag of Words (CBOW)\n",
    "* These models use shallow neural networks to learn word embeddings. Skip-gram predicts context words given a target word, while CBOW predicts a target word given its context. These models use shallow neural networks to learn word embeddings. Skip-gram predicts context words given a target word, while CBOW predicts a target word given its context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33151db6",
   "metadata": {
    "papermill": {
     "duration": 0.009462,
     "end_time": "2024-02-10T17:22:08.014770",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.005308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**CBOW**\n",
    "* Objective: The main objective of CBOW is to predict a target word given its context (surrounding words). It learns to represent words in a continuous vector space based on their distributional semantics.\n",
    "* Architecture: CBOW uses a neural network with a single hidden layer. The input layer and output layer are typically equal to the size of the vocabulary, and the hidden layer has a much smaller dimension, often referred to as the embedding dimension.\n",
    "* Input and Output: The input to the CBOW model is a set of context words represented as one-hot vectors (binary vectors with a 1 at the index corresponding to the word's position in the vocabulary). The output is the target word's one-hot vector.\n",
    "* Context Window: The context window is a fixed-size window of surrounding words used to predict the target word. The model is trained to predict the target word based on the words within this context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89b1efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:08.035789Z",
     "iopub.status.busy": "2024-02-10T17:22:08.035404Z",
     "iopub.status.idle": "2024-02-10T17:22:08.052653Z",
     "shell.execute_reply": "2024-02-10T17:22:08.051558Z"
    },
    "papermill": {
     "duration": 0.030233,
     "end_time": "2024-02-10T17:22:08.054875",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.024642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hat</th>\n",
       "      <th>the</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>in</th>\n",
       "      <th>quick</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>fox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hat  the  brown  cat  in  quick  blue  is  fox\n",
       "0    1    2      0    1   1      0     0   0    0\n",
       "1    0    1      1    0   0      1     0   0    1\n",
       "2    1    1      0    0   0      0     1   1    0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_wrd_Doc(wrd,doc):\n",
    "    i=0\n",
    "    for w in doc.lower().split(' '):\n",
    "        if wrd == w:\n",
    "            i = i+1\n",
    "    return i\n",
    "    \n",
    "cw_df = pd.DataFrame(columns=list(wrds)) #empty dataframe initialized with words column headers\n",
    "freq_lst=[] #empty list for each column to save word frequencies in each document\n",
    "for c in tf_df.columns:\n",
    "    freq_lst=[]#empty the list\n",
    "    for d in Docs:\n",
    "        freq_lst.append(count_wrd_Doc(c,d))#append the frequency of word in document d\n",
    "    cw_df[c]=freq_lst #assign values to column\n",
    "cw_df #display the datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846940f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:08.076457Z",
     "iopub.status.busy": "2024-02-10T17:22:08.076099Z",
     "iopub.status.idle": "2024-02-10T17:22:08.085227Z",
     "shell.execute_reply": "2024-02-10T17:22:08.084251Z"
    },
    "papermill": {
     "duration": 0.022438,
     "end_time": "2024-02-10T17:22:08.087667",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.065229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words representation:\n",
      "[[0 0 1 0 1 1 0 0 2]\n",
      " [0 1 0 1 0 0 0 1 1]\n",
      " [1 0 0 0 1 0 1 0 1]]\n",
      "Feature names:\n",
      "['blue' 'brown' 'cat' 'fox' 'hat' 'in' 'is' 'quick' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents to create the Bag of Words representation\n",
    "X_bow = vectorizer.fit_transform(Docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Print the Bag of Words representation\n",
    "print(\"Bag of Words representation:\")\n",
    "print(X_bow.toarray())\n",
    "print(\"Feature names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3392a2",
   "metadata": {
    "papermill": {
     "duration": 0.009537,
     "end_time": "2024-02-10T17:22:08.107234",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.097697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Skip-Grams**\n",
    "* **Objective:** The main objective of the Skip-gram model is to learn distributed representations (word embeddings) of words in a continuous vector space. It does so by predicting the context words based on a given target word.\n",
    "* **Architecture:** Skip-gram uses a neural network with a single hidden layer. The input layer and output layer are typically equal to the size of the vocabulary, and the hidden layer has a much smaller dimension, often referred to as the embedding dimension.\n",
    "* **Input and Output:** The input to the Skip-gram model is a one-hot vector representing a target word (the word for which embeddings are being learned). The output is a probability distribution over the vocabulary, representing the likelihood of each word being a context word.\n",
    "* **Context Window:** During training, a context window is defined around the target word. The context words within this window are used to predict the target word. The context window provides local context information for each target word.\n",
    "* **Training Objective:** Skip-gram is trained using a supervised learning approach. The model aims to minimize the cross-entropy loss between the predicted probability distribution over the vocabulary and the actual distribution (one-hot vector of the true context word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57e443bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:08.128352Z",
     "iopub.status.busy": "2024-02-10T17:22:08.127975Z",
     "iopub.status.idle": "2024-02-10T17:22:08.137183Z",
     "shell.execute_reply": "2024-02-10T17:22:08.136075Z"
    },
    "papermill": {
     "duration": 0.022576,
     "end_time": "2024-02-10T17:22:08.139661",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.117085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document: the cat in the hat\n",
      "target: the\n",
      "context words ['cat', 'in', 'the'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the cat in the hat\n",
      "target: cat\n",
      "context words ['the', 'in', 'the', 'hat'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the cat in the hat\n",
      "target: in\n",
      "context words ['the', 'cat', 'the', 'hat'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the cat in the hat\n",
      "target: the\n",
      "context words ['the', 'cat', 'in', 'hat'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the cat in the hat\n",
      "target: hat\n",
      "context words ['cat', 'in', 'the'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the quick brown fox\n",
      "target: the\n",
      "context words ['quick', 'brown', 'fox'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the quick brown fox\n",
      "target: quick\n",
      "context words ['the', 'brown', 'fox'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the quick brown fox\n",
      "target: brown\n",
      "context words ['the', 'quick', 'fox'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the quick brown fox\n",
      "target: fox\n",
      "context words ['the', 'quick', 'brown'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the hat is blue\n",
      "target: the\n",
      "context words ['hat', 'is', 'blue'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the hat is blue\n",
      "target: hat\n",
      "context words ['the', 'is', 'blue'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the hat is blue\n",
      "target: is\n",
      "context words ['the', 'hat', 'blue'] ,window_size 3\n",
      "----------------------------------------\n",
      "document: the hat is blue\n",
      "target: blue\n",
      "context words ['the', 'hat', 'is'] ,window_size 3\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate training pairs (target word, context word)\n",
    "window_size = 3# the window specifies the context words size neigbored the target word \n",
    "training_pairs = []\n",
    "context_words = []#the context words for w=3 i-3,i-2,i-1,i+1,i+2,i+3\n",
    "for d in Docs:\n",
    "    t=[]\n",
    "    c=[]\n",
    "    for i, target_word in enumerate(d.lower().split(' ')):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(d.lower().split(' ')), i + window_size + 1)\n",
    "        c = [d.lower().split(' ')[j] for j in range(start, end) if j != i]\n",
    "        for context_word in c:\n",
    "            t.append((target_word, context_word))\n",
    "        print(\"document:\",d.lower())\n",
    "        print(\"target:\",target_word)\n",
    "        print(\"context words\",c,\",window_size\",window_size)\n",
    "        print(\"----------------------------------------\")\n",
    "    training_pairs.append(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60814e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:08.161448Z",
     "iopub.status.busy": "2024-02-10T17:22:08.161065Z",
     "iopub.status.idle": "2024-02-10T17:22:08.169724Z",
     "shell.execute_reply": "2024-02-10T17:22:08.168658Z"
    },
    "papermill": {
     "duration": 0.021964,
     "end_time": "2024-02-10T17:22:08.171793",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.149829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 'cat'),\n",
       "  ('the', 'in'),\n",
       "  ('the', 'the'),\n",
       "  ('cat', 'the'),\n",
       "  ('cat', 'in'),\n",
       "  ('cat', 'the'),\n",
       "  ('cat', 'hat'),\n",
       "  ('in', 'the'),\n",
       "  ('in', 'cat'),\n",
       "  ('in', 'the'),\n",
       "  ('in', 'hat'),\n",
       "  ('the', 'the'),\n",
       "  ('the', 'cat'),\n",
       "  ('the', 'in'),\n",
       "  ('the', 'hat'),\n",
       "  ('hat', 'cat'),\n",
       "  ('hat', 'in'),\n",
       "  ('hat', 'the')],\n",
       " [('the', 'quick'),\n",
       "  ('the', 'brown'),\n",
       "  ('the', 'fox'),\n",
       "  ('quick', 'the'),\n",
       "  ('quick', 'brown'),\n",
       "  ('quick', 'fox'),\n",
       "  ('brown', 'the'),\n",
       "  ('brown', 'quick'),\n",
       "  ('brown', 'fox'),\n",
       "  ('fox', 'the'),\n",
       "  ('fox', 'quick'),\n",
       "  ('fox', 'brown')],\n",
       " [('the', 'hat'),\n",
       "  ('the', 'is'),\n",
       "  ('the', 'blue'),\n",
       "  ('hat', 'the'),\n",
       "  ('hat', 'is'),\n",
       "  ('hat', 'blue'),\n",
       "  ('is', 'the'),\n",
       "  ('is', 'hat'),\n",
       "  ('is', 'blue'),\n",
       "  ('blue', 'the'),\n",
       "  ('blue', 'hat'),\n",
       "  ('blue', 'is')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs # this is the pairs of training formed the target and context word pairs according to context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ad31f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:08.194166Z",
     "iopub.status.busy": "2024-02-10T17:22:08.193797Z",
     "iopub.status.idle": "2024-02-10T17:22:08.547309Z",
     "shell.execute_reply": "2024-02-10T17:22:08.545203Z"
    },
    "papermill": {
     "duration": 0.367409,
     "end_time": "2024-02-10T17:22:08.549634",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.182225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: -2.6559403839615046\n",
      "-------------------------------\n",
      "Vector for 'hat': [-0.0003036   0.00181339 -0.00033354  0.00112956  0.00113328 -0.00022667\n",
      "  0.00074363  0.00093892 -0.00031782  0.00055166]\n",
      "Vector for 'cat': [ 2.88695730e-04 -8.11029337e-04  1.87857710e-04 -6.60917472e-04\n",
      " -5.06061906e-04  1.12268329e-04 -2.99396961e-04 -6.03785820e-04\n",
      "  7.11271142e-05 -1.83907244e-04]\n",
      "Vector for 'the': [-3.45655989e-05  1.96401729e-04 -5.65923096e-05  1.21789030e-04\n",
      "  1.22351982e-04 -3.77409144e-05  7.35176795e-05  9.26347037e-05\n",
      " -4.00820152e-05  5.57996714e-05]\n",
      "Vector for 'in': [-1.39003618e-05 -7.97952342e-04  1.61309415e-04 -3.40975922e-04\n",
      " -4.98458313e-04  1.23056644e-04 -3.42012430e-04 -2.07140891e-04\n",
      "  2.25017830e-04 -2.94917755e-04]\n",
      "-------------------------------\n",
      "Epoch 0, Loss: -2.457146763850078\n",
      "-------------------------------\n",
      "Vector for 'fox': [-3.13487811e-04  5.07488525e-04  1.92003291e-03 -2.44216659e-03\n",
      " -8.80767314e-04  1.15229227e-03 -1.67820305e-03  1.64557122e-03\n",
      " -1.44587014e-03 -1.34856272e-05]\n",
      "Vector for 'brown': [ 0.00026375 -0.00155659 -0.00204374  0.00104694 -0.00013535  0.00074056\n",
      " -0.00050795  0.00147138 -0.00132968  0.00071873]\n",
      "Vector for 'the': [-0.00044094 -0.00189881 -0.00029274 -0.00171807  0.0028748  -0.00176419\n",
      "  0.00186494  0.00028017  0.00149725  0.00082991]\n",
      "Vector for 'quick': [ 0.00049023  0.00293828  0.00040098  0.0031299  -0.00182234 -0.0001643\n",
      "  0.00036501 -0.00343057  0.00132179 -0.00153368]\n",
      "-------------------------------\n",
      "Epoch 0, Loss: -1.7710075443153288\n",
      "-------------------------------\n",
      "Vector for 'hat': [ 0.00210006  0.00036905 -0.00310549 -0.00326414  0.00014777 -0.00014803\n",
      "  0.00090061  0.00128729  0.00150416  0.00241743]\n",
      "Vector for 'blue': [ 1.64585521e-04  2.39285194e-03  2.52248995e-04 -1.97953670e-04\n",
      " -7.05468612e-05 -1.88522216e-03  5.92095878e-05 -2.61983738e-03\n",
      "  7.34588637e-04 -2.56404526e-03]\n",
      "Vector for 'the': [ 9.35939390e-04 -1.77337334e-03  2.21179570e-03  1.97685697e-03\n",
      "  4.12627465e-04  6.25680764e-05 -9.39732055e-04 -5.44676283e-04\n",
      " -9.42164623e-05  2.15960092e-03]\n",
      "Vector for 'is': [-3.17162231e-03 -1.02986832e-03  6.49748226e-04  1.49225653e-03\n",
      " -4.81732976e-04  1.98513236e-03 -2.71145731e-05  1.89896503e-03\n",
      " -2.13976986e-03 -1.94293396e-03]\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize word vectors randomly\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "word_vectors = []\n",
    "vocab=[]\n",
    "for d in Docs:\n",
    "    vocab.append((list(set(d.lower().split(' ')))))\n",
    "for v in vocab:\n",
    "    word_vectors.append({word: np.random.rand(embedding_dim) for word in v})#initialize random values vector for each word \n",
    "    \n",
    "for i in range(0,len(training_pairs)):\n",
    "    # Train the Skip-gram model\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        for target_word, context_word in training_pairs[i]:\n",
    "            # Forward pass\n",
    "            input_vector = word_vectors[i][target_word]\n",
    "            output_vector = word_vectors[i][context_word]\n",
    "\n",
    "            # Calculate loss (using negative log likelihood)\n",
    "            error = -np.log(np.exp(np.dot(input_vector, output_vector)))\n",
    "\n",
    "            # Backward pass (update word vectors using gradient descent)\n",
    "            gradient = input_vector * np.exp(np.dot(input_vector, output_vector)) / (1 + np.exp(np.dot(input_vector, output_vector)))\n",
    "            word_vectors[i][target_word] -= learning_rate * gradient\n",
    "            word_vectors[i][context_word] -= learning_rate * gradient\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {error}\")\n",
    "    print('-------------------------------')\n",
    "    #word vectors\n",
    "    for word, vector in word_vectors[i].items():\n",
    "        print(f\"Vector for '{word}': {vector}\")\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0067b",
   "metadata": {
    "papermill": {
     "duration": 0.010172,
     "end_time": "2024-02-10T17:22:08.570426",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.560254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TensorFlow Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "210ba41e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-10T17:22:08.593600Z",
     "iopub.status.busy": "2024-02-10T17:22:08.592699Z",
     "iopub.status.idle": "2024-02-10T17:22:15.830588Z",
     "shell.execute_reply": "2024-02-10T17:22:15.829313Z"
    },
    "papermill": {
     "duration": 7.252079,
     "end_time": "2024-02-10T17:22:15.833013",
     "exception": false,
     "start_time": "2024-02-10T17:22:08.580934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'hat': [-0.41251007  0.33486104 -0.34530315  0.38360247  0.38497317 -0.38745227\n",
      "  0.3704791   0.4053158  -0.35356373  0.37054896]\n",
      "Vector for 'cat': [-0.378664    0.33768824 -0.39989138  0.36110294  0.37351367 -0.36743233\n",
      "  0.38150832  0.35141283 -0.3994842   0.4010619 ]\n",
      "Vector for 'the': [-0.3817028   0.34179202 -0.33527666  0.40051955  0.39800316 -0.42337108\n",
      "  0.3575181   0.372609   -0.35564923  0.40537363]\n",
      "Vector for 'in': [-0.37282875  0.4038641  -0.37826753  0.33816996  0.38914675 -0.4147259\n",
      "  0.34724402  0.3992696  -0.3810232   0.3306855 ]\n",
      "-----------------------------------------------------------\n",
      "Vector for 'fox': [-0.3531021  -0.36173883 -0.31544584 -0.3590148  -0.38313895 -0.35300878\n",
      " -0.37897828 -0.3891713  -0.37717873 -0.42813072]\n",
      "Vector for 'brown': [-0.37179777 -0.3803115  -0.41396692 -0.35756832 -0.36743087 -0.37215778\n",
      " -0.3435487  -0.3800809  -0.3913156  -0.38554367]\n",
      "Vector for 'the': [-0.38982576 -0.37258485 -0.40828994 -0.3569733  -0.42516893 -0.3648791\n",
      " -0.39222708 -0.37349015 -0.33374617 -0.34728268]\n",
      "Vector for 'quick': [-0.34001073 -0.4052905  -0.37572527 -0.4228738  -0.36205122 -0.36836168\n",
      " -0.35346916 -0.35282916 -0.3470294  -0.35487732]\n",
      "-----------------------------------------------------------\n",
      "Vector for 'hat': [ 0.3727032  -0.36051834 -0.39647838 -0.338679   -0.38563445 -0.33661148\n",
      " -0.38818505 -0.36033526 -0.397147    0.4028416 ]\n",
      "Vector for 'blue': [ 0.36503863 -0.41410363 -0.38843086 -0.38511884 -0.39392936 -0.38053852\n",
      " -0.37082246 -0.39888084 -0.2838307   0.3823766 ]\n",
      "Vector for 'the': [ 0.39485744 -0.3250419  -0.35893604 -0.360347   -0.37853628 -0.39376196\n",
      " -0.43404344 -0.3832037  -0.36837995  0.40292686]\n",
      "Vector for 'is': [ 0.34998727 -0.37058777 -0.40881845 -0.37830412 -0.38508132 -0.40495035\n",
      " -0.42868835 -0.39299786 -0.41232863  0.3735244 ]\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dot, Dense,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create a vocabulary with unique words and indices\n",
    "vocab = []\n",
    "for doc in Docs:\n",
    "    wrds=list(set(doc.lower().split()))\n",
    "    vocab.append({w:wrds.index(w) for w in wrds})\n",
    "reverse_vocab = []\n",
    "for v in vocab:\n",
    "    reverse_vocab.append({idx: word for word, idx in v.items()})\n",
    "\n",
    "# Generate training pairs (target word, context word)\n",
    "training=[]\n",
    "for d,v in zip(Docs,vocab):\n",
    "    training_pairs=[]\n",
    "    for i, target_word in enumerate(d.lower().split(' ')):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(d.lower().split(' ')), i + window_size + 1)\n",
    "        context_words = [d.lower().split(' ')[j] for j in range(start, end) if j != i]\n",
    "        for context_word in context_words:\n",
    "            training_pairs.append((v[target_word], v[context_word]))\n",
    "    training.append(training_pairs)\n",
    "    \n",
    "    \n",
    "# Define the Skip-gram model using TensorFlow\n",
    "for i,v,t_p in zip(range(0,len(Docs)),vocab,training):\n",
    "    target_word_input = tf.keras.layers.Input(shape=(1,), name=\"target_word\")\n",
    "    context_word_input = tf.keras.layers.Input(shape=(1,), name=\"context_word\")\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=len(v), output_dim=embedding_dim)\n",
    "    target_word_embedding = embedding_layer(target_word_input)#obtain respective embeddings\n",
    "    context_word_embedding = embedding_layer(context_word_input)#obtain respective embeddings\n",
    "    dot_product = Dot(axes=1, normalize=False)([target_word_embedding, context_word_embedding])#captures relations and similarity between context and target words\n",
    "    #dot_product = Dot(axes=2)([target_word_embedding, context_word_embedding])\n",
    "    output_layer = Dense(1, activation='sigmoid')(Flatten()(dot_product))  # Flatten the output\n",
    "\n",
    "    model = Model(inputs=[target_word_input, context_word_input], outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    \n",
    "    # Train the Skip-gram model\n",
    "    target_words = np.array([pair[0] for pair in t_p], dtype=np.int32)\n",
    "    context_words = np.array([pair[1] for pair in t_p], dtype=np.int32)\n",
    "\n",
    "    labels = np.array([1] * len(t_p), dtype=np.float32)# Positive labels for all training pairs\n",
    "    \n",
    "    model.fit({'target_word': target_words, 'context_word': context_words}, labels, epochs=500,verbose=0)\n",
    "\n",
    "    # Access word vectors\n",
    "    word_vectors = embedding_layer.get_weights()[0]\n",
    "    for idx, word in reverse_vocab[i].items():\n",
    "        print(f\"Vector for '{word}': {word_vectors[idx]}\")\n",
    "    \n",
    "    keras.utils.plot_model(model,show_shapes=True,show_layer_names=True)\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e9ef62",
   "metadata": {
    "papermill": {
     "duration": 0.010567,
     "end_time": "2024-02-10T17:22:15.855053",
     "exception": false,
     "start_time": "2024-02-10T17:22:15.844486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.276849,
   "end_time": "2024-02-10T17:22:17.390074",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-10T17:21:48.113225",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
